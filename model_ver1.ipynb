{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime \n",
    "start_real = datetime.now()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dropout, Dense, concatenate, LSTM, GRU, Embedding, Flatten, Activation\n",
    "# from keras.layers import Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "# set seed\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(Y, Y_pred):\n",
    "    assert Y.shape == Y_pred.shape\n",
    "    return np.sqrt(np.mean(np.square(Y_pred - Y )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1482535, 8) (693359, 7)\n",
      "CPU times: user 8.88 s, sys: 862 ms, total: 9.74 s\n",
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_df = pd.read_csv('../mercari/train.tsv', sep='\\t')\n",
    "test_df = pd.read_csv('../mercari/test.tsv', sep='\\t')\n",
    "print(train_df.shape,test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1481661, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove low prices\n",
    "train_df = train_df.drop(train_df[(train_df.price < 3.0)].index)\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.5 s, sys: 112 ms, total: 12.6 s\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get name and description lengths\n",
    "def wordCount(text):\n",
    "    try:\n",
    "        if text == 'No description yet':\n",
    "            return 0\n",
    "        else:\n",
    "            text = text.lower()\n",
    "            words = [w for w in text.split(\" \")]\n",
    "            return len(words)\n",
    "    except: \n",
    "        return 0\n",
    "train_df['desc_len'] = train_df['item_description'].apply(lambda x: wordCount(x))\n",
    "test_df['desc_len'] = test_df['item_description'].apply(lambda x: wordCount(x))\n",
    "train_df['name_len'] = train_df['name'].apply(lambda x: wordCount(x))\n",
    "test_df['name_len'] = test_df['name'].apply(lambda x: wordCount(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.13 s, sys: 877 ms, total: 9 s\n",
      "Wall time: 9.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# split category name into 3 parts\n",
    "def split_cat(text):\n",
    "    try: return text.split(\"/\")\n",
    "    except: return (\"No Label\", \"No Label\", \"No Label\")\n",
    "train_df['subcat_0'], train_df['subcat_1'], train_df['subcat_2'] = \\\n",
    "zip(*train_df['category_name'].apply(lambda x: split_cat(x)))\n",
    "test_df['subcat_0'], test_df['subcat_1'], test_df['subcat_2'] = \\\n",
    "zip(*test_df['category_name'].apply(lambda x: split_cat(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137342\n",
      "CPU times: user 1min 17s, sys: 699 ms, total: 1min 17s\n",
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "full_set = pd.concat([train_df,test_df])\n",
    "all_brands = set(full_set['brand_name'].values)\n",
    "train_df.brand_name.fillna(value=\"missing\",inplace=True)\n",
    "test_df.brand_name.fillna(value=\"missing\",inplace=True)\n",
    "\n",
    "premissing = len(train_df.loc[train_df['brand_name']=='missing'])\n",
    "def brandfinder(line):\n",
    "    brand = line[0]\n",
    "    name = line[1]\n",
    "    namesplit = name.split(' ')\n",
    "    if brand == 'missing':\n",
    "        for x in namesplit:\n",
    "            if x in all_brands:\n",
    "                return name\n",
    "    if name in all_brands:\n",
    "        return name\n",
    "    return brand\n",
    "train_df['brand_name'] = train_df[['brand_name','name']].apply(lambda x:brandfinder(x),axis=1)\n",
    "test_df['brand_name'] = test_df[['brand_name','name']].apply(lambda x:brandfinder(x),axis=1)\n",
    "found = premissing - len(train_df.loc[train_df['brand_name']=='missing'])\n",
    "print(found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 1466844 examples\n",
      "Validating on 14817 examples\n",
      "Testing on 693359 examples\n"
     ]
    }
   ],
   "source": [
    "# Scale target variable to log.\n",
    "train_df[\"target\"] = np.log1p(train_df.price)\n",
    "\n",
    "# Split training examples into train/dev examples.\n",
    "train_df, dev_df = train_test_split(train_df, random_state=123, train_size=0.99)\n",
    "\n",
    "# Calculate number of train/dev/test examples.\n",
    "n_trains = train_df.shape[0]\n",
    "n_devs = dev_df.shape[0]\n",
    "n_tests = test_df.shape[0]\n",
    "print(\"Training on\", n_trains, \"examples\")\n",
    "print(\"Validating on\", n_devs, \"examples\")\n",
    "print(\"Testing on\", n_tests, \"examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train - dev - test data for easy to handle\n",
    "full_df = pd.concat([train_df, dev_df, test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling missing data...\n",
      "1    Electronics/Computers & Tablets/Components & P...\n",
      "1              Other/Office supplies/Shipping Supplies\n",
      "Name: category_name, dtype: object\n",
      "CPU times: user 891 ms, sys: 53.1 ms, total: 944 ms\n",
      "Wall time: 956 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Filling missing values\n",
    "def fill_missing_values(df):\n",
    "    df.category_name.fillna(value=\"missing\", inplace=True)\n",
    "    df.brand_name.fillna(value=\"missing\", inplace=True)\n",
    "    df.item_description.fillna(value=\"missing\", inplace=True)\n",
    "    df.item_description.replace('No description yet',\"missing\", inplace=True)\n",
    "    return df\n",
    "\n",
    "print(\"Filling missing data...\")\n",
    "full_df = fill_missing_values(full_df)\n",
    "print(full_df.category_name[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing categorical data...\n",
      "CPU times: user 58.9 s, sys: 654 ms, total: 59.5 s\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Processing categorical data...\")\n",
    "le = LabelEncoder()\n",
    "# full_df.category = full_df.category_name\n",
    "le.fit(full_df.category_name)\n",
    "full_df['category'] = le.transform(full_df.category_name)\n",
    "\n",
    "le.fit(full_df.brand_name)\n",
    "full_df.brand_name = le.transform(full_df.brand_name)\n",
    "\n",
    "le.fit(full_df.subcat_0)\n",
    "full_df.subcat_0 = le.transform(full_df.subcat_0)\n",
    "\n",
    "le.fit(full_df.subcat_1)\n",
    "full_df.subcat_1 = le.transform(full_df.subcat_1)\n",
    "\n",
    "le.fit(full_df.subcat_2)\n",
    "full_df.subcat_2 = le.transform(full_df.subcat_2)\n",
    "\n",
    "del le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming text data to sequences...\n",
      "   Fitting tokenizer...\n",
      "   Transforming text to sequences...\n",
      "CPU times: user 3min 54s, sys: 4.63 s, total: 3min 59s\n",
      "Wall time: 4min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Transforming text data to sequences...\")\n",
    "raw_text = np.hstack([full_df.item_description.str.lower(), full_df.name.str.lower(), full_df.category_name.str.lower()])\n",
    "\n",
    "print(\"   Fitting tokenizer...\")\n",
    "tok_raw = Tokenizer()\n",
    "tok_raw.fit_on_texts(raw_text)\n",
    "\n",
    "print(\"   Transforming text to sequences...\")\n",
    "full_df['seq_item_description'] = tok_raw.texts_to_sequences(full_df.item_description.str.lower())\n",
    "full_df['seq_name'] = tok_raw.texts_to_sequences(full_df.name.str.lower())\n",
    "# full_df['seq_category'] = tok_raw.texts_to_sequences(full_df.category_name.str.lower())\n",
    "\n",
    "del tok_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>category_name</th>\n",
       "      <th>desc_len</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>item_description</th>\n",
       "      <th>name</th>\n",
       "      <th>name_len</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>subcat_0</th>\n",
       "      <th>subcat_1</th>\n",
       "      <th>subcat_2</th>\n",
       "      <th>target</th>\n",
       "      <th>test_id</th>\n",
       "      <th>train_id</th>\n",
       "      <th>category</th>\n",
       "      <th>seq_item_description</th>\n",
       "      <th>seq_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>542290</th>\n",
       "      <td>31230</td>\n",
       "      <td>Women/Tops &amp; Blouses/Blouse</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>~NWOT Sorry I'm Late T-Shirt ~Camo Long Sleeve...</td>\n",
       "      <td>Bundle For Susan's Gear</td>\n",
       "      <td>4</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>104</td>\n",
       "      <td>97</td>\n",
       "      <td>3.737670</td>\n",
       "      <td>NaN</td>\n",
       "      <td>542290.0</td>\n",
       "      <td>1277</td>\n",
       "      <td>[350, 1075, 305, 4504, 71, 101, 1222, 144, 240...</td>\n",
       "      <td>[30, 4, 62703, 1077]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201201</th>\n",
       "      <td>22723</td>\n",
       "      <td>Electronics/TV, Audio &amp; Surveillance/Headphones</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>missing</td>\n",
       "      <td>Powerbeats2 Wireless Beats by Dre Bundle</td>\n",
       "      <td>6</td>\n",
       "      <td>105.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>391</td>\n",
       "      <td>4.663439</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201201.0</td>\n",
       "      <td>103</td>\n",
       "      <td>[83]</td>\n",
       "      <td>[11737, 921, 1638, 129, 3161, 30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980192</th>\n",
       "      <td>177922</td>\n",
       "      <td>Beauty/Makeup/Lips</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>Contains: 1 Matte Liquid Lipstick (0.11 fl oz....</td>\n",
       "      <td>Kylie | Smile | Lip Kit</td>\n",
       "      <td>6</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>492</td>\n",
       "      <td>3.401197</td>\n",
       "      <td>NaN</td>\n",
       "      <td>980192.0</td>\n",
       "      <td>28</td>\n",
       "      <td>[1060, 32, 295, 478, 361, 133, 365, 599, 194, ...</td>\n",
       "      <td>[530, 3770, 251, 434]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167901</th>\n",
       "      <td>12296</td>\n",
       "      <td>Women/Sweaters/Hooded</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>missing</td>\n",
       "      <td>Aeropostale size L hoodie</td>\n",
       "      <td>4</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>97</td>\n",
       "      <td>416</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1167901.0</td>\n",
       "      <td>1262</td>\n",
       "      <td>[83]</td>\n",
       "      <td>[1653, 7, 225, 269]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986990</th>\n",
       "      <td>171601</td>\n",
       "      <td>Home/Kitchen &amp; Dining/Bakeware</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>SEE ALL PHOTOS PLEASE Has some scratches due t...</td>\n",
       "      <td>[rm] WILTON - BATMAN CAKE PAN</td>\n",
       "      <td>6</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>51</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>NaN</td>\n",
       "      <td>986990.0</td>\n",
       "      <td>522</td>\n",
       "      <td>[177, 44, 604, 67, 73, 181, 472, 731, 9, 588, ...</td>\n",
       "      <td>[22, 9782, 1507, 2070, 2742]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         brand_name                                    category_name  \\\n",
       "542290        31230                      Women/Tops & Blouses/Blouse   \n",
       "201201        22723  Electronics/TV, Audio & Surveillance/Headphones   \n",
       "980192       177922                               Beauty/Makeup/Lips   \n",
       "1167901       12296                            Women/Sweaters/Hooded   \n",
       "986990       171601                   Home/Kitchen & Dining/Bakeware   \n",
       "\n",
       "         desc_len  item_condition_id  \\\n",
       "542290         20                  3   \n",
       "201201          0                  2   \n",
       "980192        129                  1   \n",
       "1167901         0                  3   \n",
       "986990         17                  2   \n",
       "\n",
       "                                          item_description  \\\n",
       "542290   ~NWOT Sorry I'm Late T-Shirt ~Camo Long Sleeve...   \n",
       "201201                                             missing   \n",
       "980192   Contains: 1 Matte Liquid Lipstick (0.11 fl oz....   \n",
       "1167901                                            missing   \n",
       "986990   SEE ALL PHOTOS PLEASE Has some scratches due t...   \n",
       "\n",
       "                                             name  name_len  price  shipping  \\\n",
       "542290                    Bundle For Susan's Gear         4   41.0         0   \n",
       "201201   Powerbeats2 Wireless Beats by Dre Bundle         6  105.0         1   \n",
       "980192                    Kylie | Smile | Lip Kit         6   29.0         0   \n",
       "1167901                 Aeropostale size L hoodie         4    9.0         0   \n",
       "986990              [rm] WILTON - BATMAN CAKE PAN         6   11.0         0   \n",
       "\n",
       "         subcat_0  subcat_1  subcat_2    target  test_id   train_id  category  \\\n",
       "542290         10       104        97  3.737670      NaN   542290.0      1277   \n",
       "201201          1       100       391  4.663439      NaN   201201.0       103   \n",
       "980192          0        63       492  3.401197      NaN   980192.0        28   \n",
       "1167901        10        97       416  2.302585      NaN  1167901.0      1262   \n",
       "986990          3        60        51  2.484907      NaN   986990.0       522   \n",
       "\n",
       "                                      seq_item_description  \\\n",
       "542290   [350, 1075, 305, 4504, 71, 101, 1222, 144, 240...   \n",
       "201201                                                [83]   \n",
       "980192   [1060, 32, 295, 478, 361, 133, 365, 599, 194, ...   \n",
       "1167901                                               [83]   \n",
       "986990   [177, 44, 604, 67, 73, 181, 472, 731, 9, 588, ...   \n",
       "\n",
       "                                  seq_name  \n",
       "542290                [30, 4, 62703, 1077]  \n",
       "201201   [11737, 921, 1638, 129, 3161, 30]  \n",
       "980192               [530, 3770, 251, 434]  \n",
       "1167901                [1653, 7, 225, 269]  \n",
       "986990        [22, 9782, 1507, 2070, 2742]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "883\n"
     ]
    }
   ],
   "source": [
    "# define constants to use when define RNN model\n",
    "MAX_NAME_SEQ = 10 #17\n",
    "MAX_ITEM_DESC_SEQ = 75 #269\n",
    "MAX_CATEGORY_SEQ = 8 #8\n",
    "MAX_TEXT = np.max([\n",
    "    np.max(full_df.seq_name.max()),\n",
    "    np.max(full_df.seq_item_description.max()),\n",
    "#     np.max(full_df.seq_category.max()),\n",
    "]) + 100\n",
    "MAX_CATEGORY = np.max(full_df.category.max()) + 1\n",
    "MAX_BRAND = np.max(full_df.brand_name.max()) + 1\n",
    "MAX_CONDITION = np.max(full_df.item_condition_id.max()) + 1\n",
    "MAX_DESC_LEN = np.max(full_df.desc_len.max()) + 1\n",
    "MAX_NAME_LEN = np.max(full_df.name_len.max()) + 1\n",
    "MAX_SUBCAT_0 = np.max(full_df.subcat_0.max()) + 1\n",
    "MAX_SUBCAT_1 = np.max(full_df.subcat_1.max()) + 1\n",
    "MAX_SUBCAT_2 = np.max(full_df.subcat_2.max()) + 1\n",
    "print(MAX_SUBCAT_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.8 s, sys: 4.85 s, total: 33.6 s\n",
      "Wall time: 35.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get data for RNN model\n",
    "def get_rnn_data(dataset):\n",
    "    X = {\n",
    "        'name': pad_sequences(dataset.seq_name, maxlen=MAX_NAME_SEQ),\n",
    "        'item_desc': pad_sequences(dataset.seq_item_description, maxlen=MAX_ITEM_DESC_SEQ),\n",
    "        'brand_name': np.array(dataset.brand_name),\n",
    "        'category': np.array(dataset.category),\n",
    "#         'category_name': pad_sequences(dataset.seq_category, maxlen=MAX_CATEGORY_SEQ),\n",
    "        'item_condition': np.array(dataset.item_condition_id),\n",
    "        'num_vars': np.array(dataset[[\"shipping\"]]),\n",
    "        'desc_len': np.array(dataset[[\"desc_len\"]]),\n",
    "        'name_len': np.array(dataset[[\"name_len\"]]),\n",
    "        'subcat_0': np.array(dataset.subcat_0),\n",
    "        'subcat_1': np.array(dataset.subcat_1),\n",
    "        'subcat_2': np.array(dataset.subcat_2),\n",
    "    }\n",
    "    return X\n",
    "\n",
    "train = full_df[:n_trains]\n",
    "dev = full_df[n_trains:n_trains+n_devs]\n",
    "test = full_df[n_trains+n_devs:]\n",
    "\n",
    "X_train = get_rnn_data(train)\n",
    "Y_train = train.target.values.reshape(-1, 1)\n",
    "\n",
    "X_dev = get_rnn_data(dev)\n",
    "Y_dev = dev.target.values.reshape(-1, 1)\n",
    "\n",
    "X_test = get_rnn_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_logarithmic_error(y_true, y_pred):\n",
    "    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n",
    "    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n",
    "    return K.sqrt(K.mean(K.square(first_log - second_log), axis=-1)+0.0000001)\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)+0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "brand_name (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_condition (InputLayer)     (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "desc_len (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "name_len (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "subcat_0 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "subcat_1 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "subcat_2 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_desc (InputLayer)          (None, 75)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "name (InputLayer)               (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 10)        1791400     brand_name[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 1, 5)         30          item_condition[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 1, 5)         1230        desc_len[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 1, 5)         90          name_len[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 1, 10)        110         subcat_0[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 1, 10)        1140        subcat_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 1, 10)        8830        subcat_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 75, 60)       19321200    item_desc[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 10, 20)       6440400     name[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 10)           0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 5)            0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 5)            0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 5)            0           embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 10)           0           embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 10)           0           embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 10)           0           embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, 16)           3696        embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     (None, 8)            696         embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "num_vars (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 80)           0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "                                                                 flatten_7[0][0]                  \n",
      "                                                                 gru_1[0][0]                      \n",
      "                                                                 gru_2[0][0]                      \n",
      "                                                                 num_vars[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          41472       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          131328      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          32896       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 128)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           8256        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 64)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            65          dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 27,782,839\n",
      "Trainable params: 27,782,839\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "CPU times: user 1.16 s, sys: 110 ms, total: 1.27 s\n",
      "Wall time: 1.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# define RNN model\n",
    "# set seed again in case testing models adjustments by looping next 2 blocks\n",
    "np.random.seed(123)\n",
    "\n",
    "def new_rnn_model(lr=0.001, decay=0.0):\n",
    "    # Inputs\n",
    "    name = Input(shape=[X_train[\"name\"].shape[1]], name=\"name\")\n",
    "    item_desc = Input(shape=[X_train[\"item_desc\"].shape[1]], name=\"item_desc\")\n",
    "    brand_name = Input(shape=[1], name=\"brand_name\")\n",
    "#     category = Input(shape=[1], name=\"category\")\n",
    "#     category_name = Input(shape=[X_train[\"category_name\"].shape[1]], name=\"category_name\")\n",
    "    item_condition = Input(shape=[1], name=\"item_condition\")\n",
    "    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n",
    "    desc_len = Input(shape=[1], name=\"desc_len\")\n",
    "    name_len = Input(shape=[1], name=\"name_len\")\n",
    "    subcat_0 = Input(shape=[1], name=\"subcat_0\")\n",
    "    subcat_1 = Input(shape=[1], name=\"subcat_1\")\n",
    "    subcat_2 = Input(shape=[1], name=\"subcat_2\")\n",
    "\n",
    "    # Embeddings layers (adjust outputs to help model)\n",
    "    emb_name = Embedding(MAX_TEXT, 20)(name)\n",
    "    emb_item_desc = Embedding(MAX_TEXT, 60)(item_desc)\n",
    "    emb_brand_name = Embedding(MAX_BRAND, 10)(brand_name)\n",
    "#     emb_category_name = Embedding(MAX_TEXT, 20)(category_name)\n",
    "#     emb_category = Embedding(MAX_CATEGORY, 10)(category)\n",
    "    emb_item_condition = Embedding(MAX_CONDITION, 5)(item_condition)\n",
    "    emb_desc_len = Embedding(MAX_DESC_LEN, 5)(desc_len)\n",
    "    emb_name_len = Embedding(MAX_NAME_LEN, 5)(name_len)\n",
    "    emb_subcat_0 = Embedding(MAX_SUBCAT_0, 10)(subcat_0)\n",
    "    emb_subcat_1 = Embedding(MAX_SUBCAT_1, 10)(subcat_1)\n",
    "    emb_subcat_2 = Embedding(MAX_SUBCAT_2, 10)(subcat_2)\n",
    "    \n",
    "\n",
    "    # rnn layers (GRUs are faster than LSTMs and speed is important here)\n",
    "    rnn_layer1 = GRU(16) (emb_item_desc)\n",
    "    rnn_layer2 = GRU(8) (emb_name)\n",
    "#     rnn_layer3 = GRU(8) (emb_category_name)\n",
    "\n",
    "    # main layers\n",
    "    main_l = concatenate([\n",
    "        Flatten() (emb_brand_name)\n",
    "#         , Flatten() (emb_category)\n",
    "        , Flatten() (emb_item_condition)\n",
    "        , Flatten() (emb_desc_len)\n",
    "        , Flatten() (emb_name_len)\n",
    "        , Flatten() (emb_subcat_0)\n",
    "        , Flatten() (emb_subcat_1)\n",
    "        , Flatten() (emb_subcat_2)\n",
    "        , rnn_layer1\n",
    "        , rnn_layer2\n",
    "#         , rnn_layer3\n",
    "        , num_vars\n",
    "    ])\n",
    "    # (incressing the nodes or adding layers does not effect the time quite as much as the rnn layers)\n",
    "    main_l = Dropout(0.1)(Dense(512,kernel_initializer='normal',activation='relu') (main_l))\n",
    "    main_l = Dropout(0.1)(Dense(256,kernel_initializer='normal',activation='relu') (main_l))\n",
    "    main_l = Dropout(0.1)(Dense(128,kernel_initializer='normal',activation='relu') (main_l))\n",
    "    main_l = Dropout(0.1)(Dense(64,kernel_initializer='normal',activation='relu') (main_l))\n",
    "\n",
    "    # the output layer.\n",
    "    output = Dense(1, activation=\"linear\") (main_l)\n",
    "    \n",
    "    model = Model([name, item_desc, brand_name , item_condition, \n",
    "                   num_vars, desc_len, name_len, subcat_0, subcat_1, subcat_2], output)\n",
    "\n",
    "    optimizer = Adam(lr=lr, decay=decay)\n",
    "    # (mean squared error loss function works as well as custom functions)  \n",
    "    model.compile(loss = 'mse', optimizer = optimizer)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = new_rnn_model()\n",
    "model.summary()\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1466844 samples, validate on 14817 samples\n",
      "Epoch 1/2\n",
      "1466844/1466844 [==============================] - 1916s 1ms/step - loss: 0.2714 - val_loss: 0.1892\n",
      "Epoch 2/2\n",
      "1466844/1466844 [==============================] - 1882s 1ms/step - loss: 0.1735 - val_loss: 0.1784\n",
      "CPU times: user 2h 36s, sys: 40min 21s, total: 2h 40min 58s\n",
      "Wall time: 1h 3min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Fit RNN model to train data\n",
    "# Set hyper parameters for the model.\n",
    "BATCH_SIZE = 512 * 2\n",
    "epochs = 2\n",
    "\n",
    "# Calculate learning rate decay.\n",
    "exp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\n",
    "steps = int(len(X_train['name']) / BATCH_SIZE) * epochs\n",
    "lr_init, lr_fin = 0.005, 0.0005\n",
    "lr_decay = exp_decay(lr_init, lr_fin, steps)\n",
    "\n",
    "# Create model and fit it with training dataset.\n",
    "rnn_model = new_rnn_model(lr=lr_init, decay=lr_decay)\n",
    "rnn_model.fit(\n",
    "        X_train, Y_train, epochs=epochs, batch_size=BATCH_SIZE,\n",
    "        validation_data=(X_dev, Y_dev), verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on validation data...\n",
      " RMSLE error: 0.422374297507\n",
      "CPU times: user 4.81 s, sys: 413 ms, total: 5.22 s\n",
      "Wall time: 1.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Evaluating the model on validation data...\") \n",
    "Y_dev_preds_rnn = rnn_model.predict(X_dev, batch_size=BATCH_SIZE)\n",
    "print(\" RMSLE error:\", rmsle(Y_dev, Y_dev_preds_rnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "693359/693359 [==============================] - 76s 110us/step\n"
     ]
    }
   ],
   "source": [
    "rnn_preds = rnn_model.predict(X_test, batch_size=BATCH_SIZE, verbose=1)\n",
    "rnn_preds = np.expm1(rnn_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train - dev - test data for easy to handle\n",
    "full_df = pd.concat([train_df, dev_df, test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling missing values...\n",
      "CPU times: user 29.5 s, sys: 593 ms, total: 30 s\n",
      "Wall time: 30.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# handel missing data and convert data type to string\n",
    "# all inputs must be strings in a ridge model\n",
    "\n",
    "print(\"Handling missing values...\")\n",
    "full_df['category_name'] = full_df['category_name'].fillna('missing').astype(str)\n",
    "full_df['subcat_0'] = full_df['subcat_0'].astype(str)\n",
    "full_df['subcat_1'] = full_df['subcat_1'].astype(str)\n",
    "full_df['subcat_2'] = full_df['subcat_2'].astype(str)\n",
    "full_df['brand_name'] = full_df['brand_name'].fillna('missing').astype(str)\n",
    "full_df['shipping'] = full_df['shipping'].astype(str)\n",
    "full_df['item_condition_id'] = full_df['item_condition_id'].astype(str)\n",
    "full_df['desc_len'] = full_df['desc_len'].astype(str)\n",
    "full_df['name_len'] = full_df['name_len'].astype(str)\n",
    "full_df['item_description'] = full_df['item_description'].fillna('No description yet').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing data...\n",
      "(2175020, 323857) (1466844, 323857) (14817, 323857) (693359, 323857)\n",
      "CPU times: user 8min 10s, sys: 54.6 s, total: 9min 4s\n",
      "Wall time: 9min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# vectorizing all the data\n",
    "\n",
    "print(\"Vectorizing data...\")\n",
    "default_preprocessor = CountVectorizer().build_preprocessor()\n",
    "def build_preprocessor(field):\n",
    "    field_idx = list(full_df.columns).index(field)\n",
    "    return lambda x: default_preprocessor(x[field_idx])\n",
    "\n",
    "vectorizer = FeatureUnion([\n",
    "    ('name', CountVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=50000,\n",
    "        preprocessor=build_preprocessor('name'))),\n",
    "#     ('category_name', CountVectorizer(\n",
    "#         token_pattern='.+',\n",
    "#         preprocessor=build_preprocessor('category_name'))),\n",
    "    ('subcat_0', CountVectorizer(\n",
    "        token_pattern='.+',\n",
    "        preprocessor=build_preprocessor('subcat_0'))),\n",
    "    ('subcat_1', CountVectorizer(\n",
    "        token_pattern='.+',\n",
    "        preprocessor=build_preprocessor('subcat_1'))),\n",
    "    ('subcat_2', CountVectorizer(\n",
    "        token_pattern='.+',\n",
    "        preprocessor=build_preprocessor('subcat_2'))),\n",
    "    ('brand_name', CountVectorizer(\n",
    "        token_pattern='.+',\n",
    "        preprocessor=build_preprocessor('brand_name'))),\n",
    "    ('shipping', CountVectorizer(\n",
    "        token_pattern='\\d+',\n",
    "        preprocessor=build_preprocessor('shipping'))),\n",
    "    ('item_condition_id', CountVectorizer(\n",
    "        token_pattern='\\d+',\n",
    "        preprocessor=build_preprocessor('item_condition_id'))),\n",
    "    ('desc_len', CountVectorizer(\n",
    "        token_pattern='\\d+',\n",
    "        preprocessor=build_preprocessor('desc_len'))),\n",
    "    ('name_len', CountVectorizer(\n",
    "        token_pattern='\\d+',\n",
    "        preprocessor=build_preprocessor('name_len'))),\n",
    "    ('item_description', TfidfVectorizer(\n",
    "        ngram_range=(1, 3),\n",
    "        max_features=100000,\n",
    "        preprocessor=build_preprocessor('item_description'))),\n",
    "])\n",
    "\n",
    "X = vectorizer.fit_transform(full_df.values)\n",
    "\n",
    "X_train = X[:n_trains]\n",
    "Y_train = train_df.target.values.reshape(-1, 1)\n",
    "\n",
    "X_dev = X[n_trains:n_trains+n_devs]\n",
    "Y_dev = dev_df.target.values.reshape(-1, 1)\n",
    "\n",
    "X_test = X[n_trains+n_devs:]\n",
    "print(X.shape, X_train.shape, X_dev.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Ridge model on training examples...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Ridge' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Ridge' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# fitting ridge model on training data\n",
    "\n",
    "print(\"Fitting Ridge model on training examples...\")\n",
    "ridge_model = Ridge(\n",
    "    solver='auto', fit_intercept=True, alpha=2.0,\n",
    "    max_iter=1000, normalize=False, tol=0.01, random_state = 1,\n",
    ")\n",
    "ridge_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# fitting ridgeCV model on the training data\n",
    "ridge_modelCV = RidgeCV(\n",
    "    fit_intercept=True, alphas=[1.0],\n",
    "    normalize=False, cv = 2, scoring='neg_mean_squared_error',\n",
    ")\n",
    "ridge_modelCV.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSL error on dev set: 0.442082878114\n"
     ]
    }
   ],
   "source": [
    "# evaluating ridge model on dev data\n",
    "Y_dev_preds_ridge = ridge_model.predict(X_dev)\n",
    "Y_dev_preds_ridge = Y_dev_preds_ridge.reshape(-1, 1)\n",
    "print(\"RMSL error on dev set:\", rmsle(Y_dev, Y_dev_preds_ridge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV RMSL error on dev set: 0.439502968327\n"
     ]
    }
   ],
   "source": [
    "# evaluating CV ridge model on dev data\n",
    "Y_dev_preds_ridgeCV = ridge_modelCV.predict(X_dev)\n",
    "Y_dev_preds_ridgeCV = Y_dev_preds_ridgeCV.reshape(-1, 1)\n",
    "print(\"CV RMSL error on dev set:\", rmsle(Y_dev, Y_dev_preds_ridgeCV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 363 ms, sys: 932 ms, total: 1.29 s\n",
      "Wall time: 1.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# make prediction on test data\n",
    "ridge_preds = ridge_model.predict(X_test)\n",
    "ridge_preds = np.expm1(ridge_preds)\n",
    "ridgeCV_preds = ridge_modelCV.predict(X_test)\n",
    "ridgeCV_preds = np.expm1(ridgeCV_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating for associated model on dev data\n",
    "This combines the 3 predicts into one. Rather than take a simple average, aggregate predicts will use ratios to vary the weights of the 3 models. It also use a simple loop to run through all the possible ratios to find the best ratio on the dev set. It is not the most computationally efficient loop but it only takes 2 seconds to run so no big deal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 µs, sys: 8 µs, total: 20 µs\n",
      "Wall time: 43.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def aggregate_predicts3(Y1, Y2, Y3, ratio1, ratio2):\n",
    "    assert Y1.shape == Y2.shape\n",
    "    return Y1 * ratio1 + Y2 * ratio2 + Y3 * (1.0 - ratio1-ratio2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 579 ms, sys: 13.4 ms, total: 592 ms\n",
      "Wall time: 599 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#ratio optimum finder for 3 models\n",
    "best1 = 0\n",
    "best2 = 0\n",
    "lowest = 0.99\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        r = i*0.01\n",
    "        r2 = j*0.01\n",
    "        if r+r2 < 1.0:\n",
    "            Y_dev_preds = aggregate_predicts3(Y_dev_preds_rnn, Y_dev_preds_ridgeCV, Y_dev_preds_ridge, r, r2)\n",
    "            fpred = rmsle(Y_dev, Y_dev_preds)\n",
    "            if fpred < lowest:\n",
    "                best1 = r\n",
    "                best2 = r2\n",
    "                lowest = fpred\n",
    "#             print(str(r)+\"-RMSL error for RNN + Ridge + RidgeCV on dev set:\", fpred)\n",
    "Y_dev_preds = aggregate_predicts3(Y_dev_preds_rnn, Y_dev_preds_ridgeCV, Y_dev_preds_ridge, best1, best2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.66\n",
      "0.0\n",
      "(Best) RMSL error for RNN + Ridge + RidgeCV on dev set: 0.414890987568\n"
     ]
    }
   ],
   "source": [
    "print(best1)\n",
    "print(best2)\n",
    "print(\"(Best) RMSL error for RNN + Ridge + RidgeCV on dev set:\", rmsle(Y_dev, Y_dev_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
